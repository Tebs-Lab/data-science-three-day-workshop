{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Saving Models in TF\n",
    "\n",
    "We don't want to retrain a neural network every time we spin up a new server. Instead, we want to load a pretrained model from a file (which could live in Amazon's S3, another cloud storage service, or as a blob in a database). The following code would be written in standard python files, versioned with `git` or some other version control system, and deployed to a powerful machine with a good GPU or cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:47:14.207840: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 1.9675 - accuracy: 0.5460\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.5927 - accuracy: 0.8293\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3915 - accuracy: 0.8955\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2856 - accuracy: 0.9253\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2310 - accuracy: 0.9406\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1960 - accuracy: 0.9525\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1706 - accuracy: 0.9582\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1539 - accuracy: 0.9620\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1400 - accuracy: 0.9650\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1209 - accuracy: 0.9707\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1083 - accuracy: 0.9737\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1077 - accuracy: 0.9731\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1015 - accuracy: 0.9750\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0938 - accuracy: 0.9765\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0947 - accuracy: 0.9769\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0873 - accuracy: 0.9786\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0793 - accuracy: 0.9813\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0803 - accuracy: 0.9809\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0670 - accuracy: 0.9839\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0604 - accuracy: 0.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:47:59.038295: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: save_files/mnist-model-generic/assets\n"
     ]
    }
   ],
   "source": [
    "## Simple neural network example.\n",
    "## So far this should all look very familiar.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = 10 \n",
    "image_size = 784\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_data = training_images.reshape(training_images.shape[0], image_size) \n",
    "test_data = test_images.reshape(test_images.shape[0], image_size)\n",
    "\n",
    "training_labels = to_categorical(training_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=512, activation='relu', input_shape=(image_size,)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    \n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(rate=.3),\n",
    "    \n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(rate=.3),\n",
    "    \n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Note: No validation data. In a go-to-production setting, you'd already be confident this model will generalize\n",
    "# so there's no point in validating it. Instead, use all the available data to train!\n",
    "model.fit(training_data, training_labels, batch_size=128, epochs=20, verbose=True) \n",
    "\n",
    "# You can save the file as an .h5, which is specific to the Keras frontend for TF\n",
    "model.save('save_files/mnist-model.h5', save_format='h5')\n",
    "\n",
    "# You can also save the file in a tensorflow format that is slightly more generic\n",
    "model.save('save_files/mnist-model-generic', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models\n",
    "\n",
    "The result of your training on the GPU is a file. Part of your service deployment is now fetching the latest version of that file and putting it in the right place. Part of your server or application code now has to load the saved model into it's memory and run it. \n",
    "\n",
    "This **does require** a significant degree of integration, specifically your server code now has to be in Python and must depend on Keras. In some cases this is not a problem, in some cases it might require standing up a standalone API server in Python and having your (say) Ruby on Rails webserver make web requests to the Python server, which runs the model and returns the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18245959281921387, 0.9661999940872192]\n",
      "[0.18245959281921387, 0.9661999940872192]\n"
     ]
    }
   ],
   "source": [
    "# Loading models from save files is pretty easy. \n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "trained_loaded_model = load_model('save_files/mnist-model.h5')\n",
    "tf_trained_loaded_model = load_model('save_files/mnist-model-generic')\n",
    "\n",
    "# Loss, Accuracy\n",
    "# They'll be the same, since it's the same model being restored from two different formats.\n",
    "a = trained_loaded_model.evaluate(test_data, test_labels, verbose=False)\n",
    "print(a)\n",
    "\n",
    "b = tf_trained_loaded_model.evaluate(test_data, test_labels, verbose=False)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Checkpoints While Training\n",
    "\n",
    "Your code or computer could crash for any number of reasons at any time. If you've been training for 10 hours and the server running that training goes down but you haven't persisted the results of your training to the hard drive, then you're going to be very sad. Instead of training with `.fit` and `epochs=999999` we want to ensure that you're periodically saving the model.\n",
    "\n",
    "Keras provides a helpful callback class that can automatically persist the model during the training process based on the results. For example, this callback makes it easy to make a checkpoint of the model every time validation accuracy improves, instead of over a fixed number of epochs. This callback can also be configured to only save the weights, see  the [ModelCheckpoint Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89500, saving model to save_files/model-checkpoint.01-0.51.h5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.89500 to 0.92217, saving model to save_files/model-checkpoint.02-0.38.h5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.92217 to 0.93933, saving model to save_files/model-checkpoint.03-0.29.h5\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.93933 to 0.95700, saving model to save_files/model-checkpoint.04-0.24.h5\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.95700\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.95700 to 0.96800, saving model to save_files/model-checkpoint.06-0.14.h5\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.96800 to 0.96917, saving model to save_files/model-checkpoint.07-0.15.h5\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.96917 to 0.97117, saving model to save_files/model-checkpoint.08-0.14.h5\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.97117\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.97117\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.97117 to 0.97383, saving model to save_files/model-checkpoint.11-0.15.h5\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.97383 to 0.97417, saving model to save_files/model-checkpoint.12-0.13.h5\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.97417\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.97417 to 0.97750, saving model to save_files/model-checkpoint.14-0.15.h5\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.97750\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.97750 to 0.97783, saving model to save_files/model-checkpoint.25-0.13.h5\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.97783 to 0.97950, saving model to save_files/model-checkpoint.26-0.14.h5\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.97950\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.97950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9fe16bb50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# This string uses the same format as Python's f-strings\n",
    "filename_format = 'save_files/model-checkpoint.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "\n",
    "model_checkpointer = ModelCheckpoint(\n",
    "    filename_format,\n",
    "    monitor='val_accuracy', \n",
    "    verbose=1, \n",
    "    save_best_only=True,     # If True, the checkpoint will be replaced every time the model improves on val_accuracy.\n",
    "    save_weights_only=False, # If True the saved files will be the weights only, not the whole model.\n",
    "    mode='auto', \n",
    "    period=1 # If larger, the checkpointer will only run every n epochs.\n",
    ")\n",
    "\n",
    "fresh_model = Sequential([\n",
    "    Dense(units=512, activation='relu', input_shape=(image_size,)),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    \n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(rate=.3),\n",
    "    \n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dropout(rate=.3),\n",
    "    \n",
    "    Dense(units=num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "fresh_model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "fresh_model.fit(\n",
    "    training_data, \n",
    "    training_labels, \n",
    "    batch_size=128, \n",
    "    epochs=30, \n",
    "    verbose=False, \n",
    "    validation_split=.1,\n",
    "    callbacks=[model_checkpointer] # Here's our checkpointer!\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
